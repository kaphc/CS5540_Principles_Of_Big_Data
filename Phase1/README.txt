CS 5540 PRINCIPLES OF BIG DATA MANAGEMENT

TEAM DETAILS
Kavin Kumar Arumugam
Shivani Sivasankar

Phase 1
1. Twitter access tokens and token secrets were generated by submitting an application by using our Twitter account on developer.twitter.com
2. We downloaded twitter data by extracting 16,000 tweets on "disease outbreak" using tweepy package in python and saved the output file in the data folder as "master_tweets.csv"
3. All the hashtags and URL's in the collected tweets were extracted using our python code and the output file was saved in the data folder as "hashtags_and_url.txt"
4. The extracted hashtags and URL's text file was copied to HDFS by running "HDFS DFS -copyFromLocal hashtags_and_url.txt HDFS destination path"
5. We executed the WordCount program in Hadoop, a Map/reduce program that counts the words within the hastags and URL text file. As WordCount executes, Hadoop printed the progress in terms of Map and Reduce. Once WordCount was finished, we verified the output that was created which is in the hadoop_wordcount folder as "part-r-00000"
6. Similarly, we executed the WordCount program on Apache Spark, verified and saved the output file under the folder spark_wordcount as "part-00000"
